{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning maze example.  \n",
    "\n",
    "Red rectangle:          explorer.  \n",
    "Black rectangles:       hells       (reward = -1).  \n",
    "Yellow bin circle:      paradise    (reward = +1).  \n",
    "All other states:       ground      (reward = 0).  \n",
    "\n",
    "<img src=\"grid.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "First we import all the libraries and the Grid environment\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from grid_env import Maze #Maze environment in grid_env.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning Algorithm  \n",
    "<img src=\"algorithm.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We will start by building the Q learning algorithm, which is the brain of the agent.\n",
    "We will define the decision functions.\n",
    "'''\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "        print('Initial Q-table')\n",
    "        print(self.q_table)\n",
    "        print(\"Each column id [0, 1, 2, 3] repesents an action ['up', 'down', 'left', 'right']\")\n",
    "\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "        # action selection\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            state_action = self.q_table.loc[observation, :]\n",
    "            # some actions may have the same value, randomly choose one in these actions\n",
    "            action = np.random.choice(state_action[state_action == np.max(state_action)].index)\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        if s_ != 'terminal':\n",
    "            q_target = r + self.gamma * self.q_table.loc[s_, :].max()  # next state is not terminal\n",
    "        else:\n",
    "            q_target = r  # next state is terminal\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)  # update\n",
    "        return(self.q_table)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(\n",
    "                pd.Series(\n",
    "                    [0]*len(self.actions),\n",
    "                    index=self.q_table.columns,\n",
    "                    name=state,\n",
    "                )\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function runs the program in a loop and updates the neccessary functions\n",
    "'''\n",
    "\n",
    "def update():\n",
    "    for episode in range(100):   \n",
    "        print('Starting episode',episode)\n",
    "        # initial observation\n",
    "        observation = env.reset()\n",
    "\n",
    "        while True:\n",
    "            # fresh env\n",
    "            env.render()\n",
    "\n",
    "            # RL choose action based on observation\n",
    "            action = RL.choose_action(str(observation))\n",
    "\n",
    "            # RL take action and get next observation and reward\n",
    "            observation_, reward, done = env.step(action)\n",
    "\n",
    "            # RL learn from this transition\n",
    "            RL.learn(str(observation), action, reward, str(observation_))\n",
    "            \n",
    "\n",
    "            # swap observation\n",
    "            observation = observation_\n",
    "\n",
    "            # break while loop when end of this episode\n",
    "            if done:\n",
    "                break\n",
    "    # end of game\n",
    "    print('game over')\n",
    "    print('final Q-table')\n",
    "    print(\"Each column id [0, 1, 2, 3] repesents an action ['up', 'down', 'left', 'right']\")\n",
    "    q_table = RL.learn(str(observation), action, reward, str(observation_))\n",
    "    print(q_table)\n",
    "\n",
    "    env.destroy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Q-table\n",
      "Empty DataFrame\n",
      "Columns: [0, 1, 2, 3]\n",
      "Index: []\n",
      "Each column id [0, 1, 2, 3] repesents an action ['up', 'down', 'left', 'right']\n",
      "Starting episode 0\n",
      "Starting episode 1\n",
      "Starting episode 2\n",
      "Starting episode 3\n",
      "Starting episode 4\n",
      "Starting episode 5\n",
      "Starting episode 6\n",
      "Starting episode 7\n",
      "Starting episode 8\n",
      "Starting episode 9\n",
      "Starting episode 10\n",
      "Starting episode 11\n",
      "Starting episode 12\n",
      "Starting episode 13\n",
      "Starting episode 14\n",
      "Starting episode 15\n",
      "Starting episode 16\n",
      "Starting episode 17\n",
      "Starting episode 18\n",
      "Starting episode 19\n",
      "Starting episode 20\n",
      "Starting episode 21\n",
      "Starting episode 22\n",
      "Starting episode 23\n",
      "Starting episode 24\n",
      "Starting episode 25\n",
      "Starting episode 26\n",
      "Starting episode 27\n",
      "Starting episode 28\n",
      "Starting episode 29\n",
      "Starting episode 30\n",
      "Starting episode 31\n",
      "Starting episode 32\n",
      "Starting episode 33\n",
      "Starting episode 34\n",
      "Starting episode 35\n",
      "Starting episode 36\n",
      "Starting episode 37\n",
      "Starting episode 38\n",
      "Starting episode 39\n",
      "Starting episode 40\n",
      "Starting episode 41\n",
      "Starting episode 42\n",
      "Starting episode 43\n",
      "Starting episode 44\n",
      "Starting episode 45\n",
      "Starting episode 46\n",
      "Starting episode 47\n",
      "Starting episode 48\n",
      "Starting episode 49\n",
      "Starting episode 50\n",
      "Starting episode 51\n",
      "Starting episode 52\n",
      "Starting episode 53\n",
      "Starting episode 54\n",
      "Starting episode 55\n",
      "Starting episode 56\n",
      "Starting episode 57\n",
      "Starting episode 58\n",
      "Starting episode 59\n",
      "Starting episode 60\n",
      "Starting episode 61\n",
      "Starting episode 62\n",
      "Starting episode 63\n",
      "Starting episode 64\n",
      "Starting episode 65\n",
      "Starting episode 66\n",
      "Starting episode 67\n",
      "Starting episode 68\n",
      "Starting episode 69\n",
      "Starting episode 70\n",
      "Starting episode 71\n",
      "Starting episode 72\n",
      "Starting episode 73\n",
      "Starting episode 74\n",
      "Starting episode 75\n",
      "Starting episode 76\n",
      "Starting episode 77\n",
      "Starting episode 78\n",
      "Starting episode 79\n",
      "Starting episode 80\n",
      "Starting episode 81\n",
      "Starting episode 82\n",
      "Starting episode 83\n",
      "Starting episode 84\n",
      "Starting episode 85\n",
      "Starting episode 86\n",
      "Starting episode 87\n",
      "Starting episode 88\n",
      "Starting episode 89\n",
      "Starting episode 90\n",
      "Starting episode 91\n",
      "Starting episode 92\n",
      "Starting episode 93\n",
      "Starting episode 94\n",
      "Starting episode 95\n",
      "Starting episode 96\n",
      "Starting episode 97\n",
      "Starting episode 98\n",
      "Starting episode 99\n",
      "game over\n",
      "final Q-table\n",
      "Each column id [0, 1, 2, 3] repesents an action ['up', 'down', 'left', 'right']\n",
      "                                         0             1         2  \\\n",
      "[5.0, 5.0, 35.0, 35.0]        6.636651e-07  1.830722e-10  0.000162   \n",
      "[5.0, 45.0, 35.0, 75.0]       1.820939e-07  0.000000e+00  0.000000   \n",
      "[45.0, 45.0, 75.0, 75.0]      0.000000e+00 -1.990000e-02 -0.010000   \n",
      "[45.0, 5.0, 75.0, 35.0]       1.856643e-05  0.000000e+00  0.001288   \n",
      "terminal                      0.000000e+00  0.000000e+00  0.000000   \n",
      "[85.0, 5.0, 115.0, 35.0]      7.110450e-08 -3.940399e-02  0.008589   \n",
      "[5.0, 85.0, 35.0, 115.0]      0.000000e+00  0.000000e+00 -0.019900   \n",
      "[125.0, 5.0, 155.0, 35.0]     1.885407e-05  4.670202e-02  0.000735   \n",
      "[125.0, 45.0, 155.0, 75.0]    4.364210e-04  1.940679e-01  0.000000   \n",
      "[125.0, 85.0, 155.0, 115.0]   1.665934e-03  8.100000e-07  0.006838   \n",
      "[5.0, 125.0, 35.0, 155.0]     0.000000e+00  0.000000e+00  0.000000   \n",
      "[45.0, 125.0, 75.0, 155.0]   -1.000000e-02  0.000000e+00  0.000000   \n",
      "[85.0, 125.0, 115.0, 155.0]   2.970100e-02  0.000000e+00  0.000000   \n",
      "[125.0, 125.0, 155.0, 155.0]  0.000000e+00  0.000000e+00  0.000000   \n",
      "\n",
      "                                         3  \n",
      "[5.0, 5.0, 35.0, 35.0]        0.000000e+00  \n",
      "[5.0, 45.0, 35.0, 75.0]       1.830722e-10  \n",
      "[45.0, 45.0, 75.0, 75.0]      0.000000e+00  \n",
      "[45.0, 5.0, 75.0, 35.0]       1.785780e-07  \n",
      "terminal                      1.000000e-02  \n",
      "[85.0, 5.0, 115.0, 35.0]      7.412071e-06  \n",
      "[5.0, 85.0, 35.0, 115.0]      0.000000e+00  \n",
      "[125.0, 5.0, 155.0, 35.0]     5.375423e-05  \n",
      "[125.0, 45.0, 155.0, 75.0]   -1.990000e-02  \n",
      "[125.0, 85.0, 155.0, 115.0]   5.744099e-01  \n",
      "[5.0, 125.0, 35.0, 155.0]     0.000000e+00  \n",
      "[45.0, 125.0, 75.0, 155.0]    0.000000e+00  \n",
      "[85.0, 125.0, 115.0, 155.0]   0.000000e+00  \n",
      "[125.0, 125.0, 155.0, 155.0]  2.682000e-04  \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Maze()\n",
    "    RL = QLearningTable(actions=list(range(env.n_actions)))\n",
    "\n",
    "    env.after(100, update)\n",
    "    env.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
